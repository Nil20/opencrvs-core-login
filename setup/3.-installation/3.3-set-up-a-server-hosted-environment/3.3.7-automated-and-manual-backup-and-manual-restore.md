---
description: >-
  Critical information required to understand how to regularly backup and
  restore your citizen registration data in case of a server problem.
---

# 3.3.7 Automated & manual backup and manual restore

{% hint style="warning" %}
As part of **OpenCRVS v1.2.0-beta (released December 2023),** we have changed the information on this page.  As a result these pages are being re-written/deprecated.  Please return in a few days for up to date content. &#x20;
{% endhint %}

### Automated backup process to an external server

If you configured a separate backup server and used the external backup server optional SSH properties in the Ansible script in step [3.3.2 Install dependencies](3.3.2-install-dependencies.md), then every day OpenCRVS automatically backs up all databases to the following directories on the manager node.&#x20;

Every 7 days the Mongo data is deleted to save disk space.  The Ansible playbook sets this up as an option in this [step](3.3.2-install-dependencies.md).  The files are backed up to that server in the middle of the night. &#x20;

{% hint style="warning" %}
TO AVOID CITIZEN DATA LOSS... You must configure and test this automated backup process in production.  Operationallly, we highly recommend that once a week, these files should be saved to a password protected and encrypted external harddrive and stored in a secure and approved location.
{% endhint %}

### Backup files

Mongo backups are saved as mongo .gz zip files using the date of the backup here:

```
/data/backups/mongo/hearth-dev-<date>.gz
/data/backups/mongo/openhim-dev-<date>.gz
/data/backups/mongo/user-mgnt-<date>.gz
/data/backups/mongo/application-config-<date>.gz
/data/backups/mongo/metrics-<date>.gz 
/data/backups/mongo/webhooks-<date>.gz // Only exists if a webhook integration is configured
```

Elasticsearch snapshot files and indices are saved here. &#x20;

{% hint style="warning" %}
**The entire elasticsearch folder contains all snapshots and must be preserved indefinitely**
{% endhint %}

```
/data/backups/elasticsearch
```

InfluxDB backup files are saved into a date named directory here:

```
/data/backups/influxdb/<date>
```

As of OpenCRVS v1.2 Minio attachments are saved as a .gz zip file using the date of the backup here:

```
/data/backups/minio/ocrvs-<date>.tar.gz
```

### Manual backup process

You can manually run the automated backup script at any point.  SSH into your server and navigate to the following directory:

```
cd /opt/opencrvs/infrastructure/
```

The script is run like this, with the paramters explained below.  Even if you have not set up an external server that the manager node can SSH into, the files will still backup to the directory locations above.  Just the SSH part will fail.  So once the script finishes you can use rsync to copy the files manually off your server and into a local directory.

{% hint style="info" %}
If you are migrating between versions of OpenCRVS, you could use the SSH details for your new server running a newer version of OpenCRVS.
{% endhint %}

Replace and separate the \<parameters> with a space when calling the script&#x20;

```
bash ./emergency-backup-metadata.sh <SSH_USER> <SSH_HOST> <SSH_PORT> <PRODUCTION_IP> <REMOTE_DIR> <REPLICAS> <VERSION>
```

| Parameter      | Optional or Mandatory | Descrption                                                                                                                                                                                                                                             |
| -------------- | --------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| SSH\_USER      | Mandatory             | This is the SSH user for your backup server.  If running manually you can enter anything, such as: **root**                                                                                                                                            |
| SSH\_HOST      | Mandatory             | This is the IP address for your backup server.  If running manually you can enter anything, such as: 132.42.41.15                                                                                                                                      |
| SSH\_PORT      | Mandatory             | This is the SSH port for your backup server.  If running manually you can enter anything, such as: **22**                                                                                                                                              |
| PRODUCTION\_IP | Mandatory             | This is the public IP address for the manager server node for your OpenCRVS cluster.  It must be entered correctly.                                                                                                                                    |
| REMOTE\_DIR    | Mandatory             | This is the directory path on the backup server that you wish to copy the backup files into. E.G.  **/root/opencrvs-backups**.  If running manually you can enter anything.                                                                            |
| REPLICAS       | Mandatory             | The number of servers in your OpenCRVS cluster.  This value can only be equal to: 1, 3 or 5                                                                                                                                                            |
| VERSION        | Optional              | Normally, today's date is used as a suffix to name the backup files in the directories as explained above.  If you want to name the backup files something different, you can enter a string here that will be used as a replacement file name suffix. |

When complete, exit your server:

```
exit
```

This command copies files from a server to your current local directory using rsync:

```
rsync -av -r --ignore-existing  --progress <ssh-user>@<opencrvs-production-ip>:/data/backups .
```

### Restore process

To perform a restore, ensure that you have backup files in the day's folders you wish to restore from.  If this is a new environment, you would need to copy the backed up files and folders into the locations using scp or rsync. &#x20;

{% hint style="danger" %}
**You should only restore to a clean installation of OpenCRVS to avoid potential issues that may be very difficult to debug.  These commands entirely replace all data and any backups that may exist on the server with your local backup files.**
{% endhint %}



Copy an Elasticsearch backup onto the server like this:

```
rsync -av --delete --progress backups/elasticsearch <your-ssh-user>@<your-production-ip>:/data/backups/elasticsearch
```

Backup files:

All the Mongo backup zips you would like to restore from must exist in **/data/backups/mongo** folder named like this:

**hearth-dev-{date}.gz**

**openhim-dev-{date}.gz**

**user-mgnt-{date}.gz**&#x20;

**application-config-{date}.gz**&#x20;

The Elasticsearch backup folder **/data/backups/elasticsearch** must exist with all previous snapshots and indices.&#x20;

The InfluxDB backup files must exist in the **/data/backups/influxdb/{date}** folder

1. SSH into the manager node
2. Make sure you are a root user
3. cd to the /opt/opencrvs/infrastructure directory
4. Ensure that your database [secrets](3.3.6-deploy.md) are available as environment variables.  You can do this by running:

```
export ELASTICSEARCH_ADMIN_USER=elastic \
export ELASTICSEARCH_ADMIN_PASSWORD=<your elastic password> \
export MONGODB_ADMIN_USER=< your mongo username> \
export MONGODB_ADMIN_PASSWORD=<your mongo password>
```

5\. Run the following script as root but beware that **ALL DATA WILL BE REPLACED BY YOUR BACKUP DATA**

```
./emergency-restore-metadata.sh <day of the week to restore from> <number of server replicas: 1 3 or 5>
```

